#!/bin/bash
# carbon-babysitter - restart carbon procs if the loss rate between relay and cache is >50%
# intended as a short term solution to carbon-relay performance issues until we upgrade/redesign
# See: http://jira.rubiconproject.com/browse/INFRA-1141 - Address Graphite performance in SJC1
# See: http://jira.rubiconproject.com/browse/INFRA-239 - Modernize our Graphite Implementation
# c Tue Sep 22 15:37:24 PDT 2015 smb - Testing in SJC1
# u Tue Sep 22 21:32:11 PDT 2015 smb - Global deployment
# u Fri Oct  2 14:09:41 PDT 2015 smb - INFRA-1165 - Modify existing carbon-babysitter script to function for GRLs
# u Tue Oct  6 09:06:08 PDT 2015 smb - INFRA-1165 - Let carbon-babysitter perform restarts for repeated check failures
# u Thu Oct  8 13:19:52 PDT 2015 smb - INFRA-1165 - Produce Graphite metrics for exit status of script
# u Tue Oct 20 11:38:44 PDT 2015 smb - INFRA-1293 - Modify GRL logic to use metric loss versus previous day instead of CPU load, due to FD creep

# Variables
GRAPHITEHOST="graphite"
GRAPHITEPORT="3003"
HOST_UNDERSCORED=`/bin/hostname -f | tr '.' '_'`
HOSTGROUP=`hostname -f | cut -c1-9,13-17 | tr a-z A-Z | tr '.' '-'`
PREFIX="metrics.$HOSTGROUP.$HOST_UNDERSCORED"
METRIC="carbon-babysitter"
STATEDIR="/tmp/carbon-babysitter"
STATEFILE="/tmp/carbon-babysitter.state"
DATENOW=`/bin/date +%s`

# Conditional health checks
# If a Graphite Storage node is seeing less than 50% of the metrics that hit the carbon-relay reach the carbon-cache...
if [[ $HOST_UNDERSCORED == *"gra"* ]]; then
	HEALTH_METRIC="https://localhost/render/?target=divideSeries(carbon.relays.${HOST_UNDERSCORED}-a.metricsReceived,sumSeries(carbon.agents.${HOST_UNDERSCORED}-*.metricsReceived))&format=csv&from=-10minutes"
	HEALTH_THRESHOLD=2.0
	CUT_FIELD=4

# If a Graphite Listener sees a dramatic (10x) decrease in metrics from this time yesterday versus now...
elif [[ $HOST_UNDERSCORED == *"grl"* ]]; then
	HEALTH_METRIC="https://opsview-graphite.las1.fanops.net/render/?target=divideSeries(timeShift(averageSeries(carbon.relays.${HOST_UNDERSCORED}-*.metricsReceived),'1d'),averageSeries(carbon.relays.${HOST_UNDERSCORED}-*.metricsReceived))&format=csv&from=-10minutes"
	HEALTH_THRESHOLD=10.0
	CUT_FIELD=5

else
	echo "`date` CRITICAL: Unsure of health check URL for this kind of host (${HOST_UNDERSCORED}). Supported host types are GRA and GRL. Refusing to continue..."
	EXITCODE=$STATE_CRITICAL
	exit $EXITCODE
fi

# Exit codes
PREVIOUS_EXITCODE=0
STATE_OK=0
STATE_WARNING=1
STATE_CRITICAL=2
STATE_UNKNOWN=3
STATE_DEPENDENT=4

# Default to UNKNOWN if no other code is set later
EXITCODE=$STATE_UNKNOWN

# Exit right away if plugin is already running.
PIDFILE="/var/run/carbon-babysitter.pid"

if [ -e $PIDFILE ]; then
        ps -p `cat $PIDFILE` &>/dev/null
        PSEXIT="$?"
        if [ "$PSEXIT" -ne "0" ]; then
                echo "PID file found at $PIDFILE, but process not found, removing PID file, and exiting."
                rm -f $PIDFILE
        else
                echo "PID file found at $PIDFILE and running process matches that PID, exiting."
        fi
        exit $EXITCODE
fi

# Otherwise, create a pid
echo $$ > $PIDFILE
mkdir -p $STATEDIR

# For the heck of it, start the daemons (will smartly fail if already running)
/sbin/service carbon-cache start &>/dev/null; sleep 2
/sbin/service carbon-relay start &>/dev/null; sleep 1

# Tests
HEALTH_CHECK=`/usr/bin/curl -k $HEALTH_METRIC 2>/dev/null | cut -d',' -f${CUT_FIELD} | sed -e '/^M/d' | tr -d '\015' | sort -nr | head -n1`
RESTART_AGE=`/usr/bin/find ${STATEFILE} -mmin +30 2>/dev/null | wc -l`

# If we're unable to retrieve data, record this event and count prior events, force restart if needed
if [ ! $HEALTH_CHECK ]; then
	echo "`date` NOTE: Did not take action based on HEALTH_THRESHOLD = $HEALTH_THRESHOLD and HEALTH_CHECK = $HEALTH_CHECK" > ${STATEDIR}/${DATENOW}
	FAILEDCOUNT=`ls -1 ${STATEDIR}/* | wc -l`
	if [ $FAILEDCOUNT -ge 10 ]; then
		FORCERESTART=1
		rm -f ${STATEDIR}/*
	fi
fi

# Only run if the highest health check value exceeds the health threshold value within the last 10 minutes (&from=-10minutes + `sort -nr | head -n1` above)
# ... or, force a restart if the past 10 attempts to check failed
if [ $FORCERESTART ] || [ 1 -eq "$(echo "${HEALTH_CHECK} > ${HEALTH_THRESHOLD}" | bc)" ]; then
	# Only proceed with a restart if it's been more than 30 minutes since it was last restarted
	if [ -f ${STATEFILE} ] && [ ${RESTART_AGE} -ne 1 ]; then
		echo "`date` WARNING: Would restart Graphite based on HEALTH_THRESHOLD = $HEALTH_THRESHOLD and HEALTH_CHECK = $HEALTH_CHECK, but less than 30 minutes since last restart."
		STATUS="warning"
		EXITCODE=$STATE_WARNING
	else
		echo "`date` CRITICAL: Restarting Graphite based on HEALTH_THRESHOLD = $HEALTH_THRESHOLD and HEALTH_CHECK = $HEALTH_CHECK"
		/sbin/service carbon-cache stop; /sbin/service carbon-relay stop
		COUNTER=0; while [ $COUNTER -lt 60 ]; do
			ALIVE=`ps -e | egrep -c 'carbon-relay.py|carbon-cache.py'`
			if [ $ALIVE -eq 0 ]; then
				echo "`date` OK: carbon-relay and carbon-cache are stopped cleanly."; COUNTER=60
			fi
			sleep 1; COUNTER=$[$COUNTER+1]
		done	
		pkill -9 carbon-cache.py; pkill -9 carbon-relay.py
		ALIVE=`ps -e | egrep -c 'carbon-relay.py|carbon-cache.py'`
		while [ $ALIVE -ne 0 ]; do
			echo "`date` WARNING - Cannot kill carbon /sbin/services, trying again."
			pkill -9 carbon-cache.py; pkill -9 carbon-relay.py
			sleep 60;
			ALIVE=`ps -e | egrep -c 'carbon-relay.py|carbon-cache.py'`
		done
		echo "`date` OK: Starting carbon-relay and carbon-cache"
		sleep 2; /sbin/service carbon-cache start; sleep 2; /sbin/service carbon-relay start
		touch ${STATEFILE}
		STATUS="critical"
		EXITCODE=$STATE_CRITICAL
	fi
else
	echo "`date` OK: Did not take action based on HEALTH_THRESHOLD = $HEALTH_THRESHOLD and HEALTH_CHECK = $HEALTH_CHECK"
	STATUS="ok"
	EXITCODE=$STATE_OK
fi

# Nobody leaves this place without singing the blues.
SUBMISSION="$PREFIX.$METRIC"_"$STATUS 1 $DATENOW"
echo $SUBMISSION | nc -w50 $GRAPHITEHOST $GRAPHITEPORT

rm -f $PIDFILE
exit $EXITCODE

